
\subsection{Extending the Tor protocol}

We extend the Tor routing protocol described in Tor's
specifications~\cite{dingledine2018tor} and we exploit Tor's leaky-pipe circuit
topology\footnote{``Leaky pipe'' refers to the ability of the user to direct
  traffic that ends at an intermediate hop along the circuit} to exchange
payment information with each hop of the circuit. We introduce two new control
cell types: one link-level cell and one relay-level cell. The link-level cell is
used to exchange information related to the payment protocol between the Tor
client and its guard relay while the relay-level cell is used for the middle
relay and the exit relay. This subtype of relay cell is comprised of a payment
header denoting the type of payment cell, followed by a payload of payment
data. To an outside observer, payment cells are indistinguishable to normal
relay cells. Figure~\ref{fig:relay_command_mt_structure} shows the internal structure of the cell.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.38]{images/payment_cell_header.png}
    \caption{Relay Payment Cell --- Cell definition specifying the structure of
      a moneTor payment cell}
\label{fig:relay_command_mt_structure}
\end{figure}

All the bytes starting from StreamID (included) are onion encrypted. RelCMD is set to RELAY\_COMMAND\_MT, PCMD is the payment command which is different for each step of the payment protocol. If some message overflow the payload available length (495 bytes), we queue multiple cells of the same PCMD and buffer them on the receiver side to unpack the whole message.

\subsection{Pre-built Channels}
By default, Tor attempts to pre-build circuits in order to reduce latency once a
user wishes to create a data stream. Much like circuits, moneTor payment
channels are high in initial latency because of the many in-out messages in the protocol. We therefore exploit the same strategy currently used in circuit establishment by
allowing payment channels to be preemptively set up and established on clean
pre-built circuits. This dramatically reduces the effective time to first
payment. Unfortunately, excessive establishment of preemptive channels will
eventually afflict the network with unused overhead. Our implementation features
a rudimentary prediction strategy that attempts to balance this trade-off by
anticipating the number of needed channels using historical usage
information in a similar way Tor anticipates the need for a fresh circuits.
%However, our approach may not be sufficiently optimized and further
%work on this front is warranted.

\subsection{Network Scalability}
\label{subsub:scalability}
%\td{TODO: describe scalability of intermediary system and any networking
%  bottlenecks that might arise such as port limits, etc.}

In our design, we are concerned with memory consumption, kernel socket
consumption and CPU consumption. Our choice for a tripartite protocol
effectively shifts the memory consumption of opened and idle micro
channels to the intermediary nodes of the network. A more basic setup
whereby each Tor client maintains a micropayment channel with each
relay would incur an $O(n*m)$ cost with
respect to channel management complexity, with $n$ the number of Tor clients and $m$ the number of relays. By engineering an additional
intermediary layer, the complexity of moneTor channel connections is
reduced to $O(n+m)$. In our implementation, intermediary relays do not
participate in routing user streams and are tasked only with providing
payment channel services. Intermediaries devote the full extent of
their computational resources toward this task, allowing only a few
strong nodes to handle all of channel management needs of the network.

%\td{Should we emphaisize more the need to anonymousely manage circuits between
%  parties? Or is this obvious?}
%FR: looks obvious to me from the phrase below
Interactions between parties are realised within Tor circuits to allow
multiplexing of circuits over the same TCP connection. As a result, the
intermediary and the ledger must have a number of available sockets higher than
the number of relays in the network in the worst case. Since this limit is in
line with Tor's current assumption, our design inherits the same socket
consumption scalability of the greater Tor network.



\subsection{Prioritized Traffic}
\label{subsub:prioritized}

The final component of our network-oriented design addresses the need to deliver
prioritized bandwidth given an explicit signaling of premium or nonpremium
traffic. Our objective is to provide a tunable range of prioritization while
incurring as little cost as possible to average global performance. Traffic
scheduling is perhaps the most intuitive mechanism with which to implement
prioritization. However, we found in our investigation that practical
modification of the Tor scheduling infrastructure is nontrivial. A more detailed
discussion of our findings can be found in Appendix~\ref{sec:scheduling}.

Fortunately, Tor's overlay flow control mechanism provides an alternative route
to implement our desired functionality. Recall that edge nodes regulate the
traffic flux in either direction using a set of flow control windows. Roughly
speaking, these windows determine space allotted to each circuit on a relay's
scheduling queue, which in turn is positively correlated with effective
bandwidth. We implement our prioritization scheme by readjusting the windows
according to the following formula.
\begin{equation}
  window' = window(1+ \alpha(premium / premium\% - 1))
  \label{eq:flow}
\end{equation}
Here, a circuit is marked as prioritized by the bit
$premium \in \{0, 1\}$. The tunable priority benefit
$\alpha \in [0, 1)$ defines the proportion of the non-premium capacity that we wish
to transfer to premium clients. By accounting for
$premium\%s \in [0,1]$, the fraction of premium to nonpremium clients, we
can keep the total flow capacity constant. Our policy may be
implemented in one of two ways. First, each node could track the
$premium\%$ locally and dynamically adjust their own windows. This
introduces a considerable amount of added complexity with unclear
consequences on network performance. A more sound approach calls for
the Tor authorities to track the global value for $premium\%$ and
periodically broadcast static flow control windows to be used by the
entire network. We adopt the latter approach in this iteration of
moneTor.

\paragraph*{Interlude.} This concludes the design of the moneTor framework. In
the proceeding sections, we describe steps taken to iteratively select and
validate key parameters as well as the scheme as a whole. Such parameters
include: payment frequency, preemptive channel creation, and prioritization
amounts ($\alpha$). Throughout this process, the underlying objective is to
prove that we can confer qualitatively ``significant'' advantage to paid premium
users while incurring minimal overhead costs with respect to throughput, memory
usage, and latency within a realistic network environment.

%\footnote{Our
%  research in traffic prioritization is meant to demonstrate at least some crude
%  capacity for premium advantage in our models and to suggest potential avenues
%  for further study. A more definitive design for production-ready policies is
%  left for future networking-oriented research.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
