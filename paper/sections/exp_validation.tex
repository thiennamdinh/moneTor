Understanding typical Tor usage and assessing it benefits from our priority scheme is a crucial requirement.
Appendix~\ref{sec:analysis} covers a real-world Tor measurement study
that illustrates the importance of token exchange within the first few seconds of the data stream.
Having established the empirical context for a channel payment scheme, we validated our technical design via experiments performed on a prototype software implementation within the native Tor codebase.
The objective is to prove that we can deliver a qualitatively ``significant'' advantage to paid premium users while incurring minimal overhead costs for throughput, memory usage, and latency within a realistic network environment.
Due to the pre-built payment channel setup and low payment verification cost, we determine that our scheme supports the majority of observed short-lived and bursty Tor circuits in a near-fair-exchange setting.

\subsection{Prototype}

\begin{figure*}[t] \centering
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[clip, width=1.0\textwidth]{images/overhead_downloadtime.pdf} \caption{Download Time Overhead - Web + Bulk}
    \label{fig:overhead_ttlastbyte}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[clip, width=1.0\textwidth]{images/overhead_throughput.pdf}
    \caption{Throughput - Compared to baseline}
    \label{fig:overhead_throughput}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[clip, width=1.0\textwidth]{images/overhead_memory.pdf}
    \caption{Simulation Memory}
    \label{fig:overhead_shadow}
  \end{subfigure}
  \caption{Global Overhead --- Comparison of overhead in pure multicore and singlecore network.
    Figure~\ref{fig:overhead_ttlastbyte} shows two sets of time CDF curves for each file size (2 MiB and 5 MiB), Figure~\ref{fig:overhead_throughput} shoes the 5 minute moving average the simulation 10 consensus file `2018-02-03-00-00-00-consensus'.}
  \label{fig:overhead}
\end{figure*}

A substantial contribution of our research is embedded within our implementation of the moneTor framework.
The modifications, applied to Tor release version 0.3.2.10, cover
approximately fifteen thousand lines of new code across Tor's core C software.
We emphasize that the implementation is engineered solely for our experiments.
Most notably, expensive cryptographic operations such as ZKPs and commitments were simulated using methods that account for Shadow's unique virtual time management~\cite{jansen2011shadow}.
We consider both scenarios in which the simulated Tor process is running on a multicore or singlecore processor.
In the multicore case, the cryptographic operations are replaced by an ``idle'' command that allows the virtual node to complete other tasks in parallel.
In the singlecore case, cryptographic operations are simulated by looping through a series of dummy SHA256 hash operations.
Using these methods, duration of the delays were tuned to
conservatively reflect real measurements published in prior background work~\cite{green2017bolt}.\footnote{Extracted values are conservative in the sense that our zero-knowledge proofs require proving only a subset of the statements required in each corresponding Bolt zero-knowledge proof.}
Note that our prototype does not implement anything that does not help us to answer our research goals, such as coin/wallet management, extension of the Tor control protocol to manage addition asset and options, Intermediary information recovery in case of crash, etc.
Instead, the prototype serves the following purposes:

\begin{enumerate}
\item Our implementation handles nuances missing from the theoretical protocol specification. We show that there are no unexpected or prohibitive practical conflicts with the existing Tor design.
\item Our platform allows us to study the feasibility of premium circuit prioritization from a networking perspective.
\item Our platform allows us to obtain a rough factor-of-two approximation for all bandwidth, computation, and memory requirements of a real deployment, both globally and at individual nodes.
\end{enumerate}

The first design purpose is clearly qualitative and we did not discover any insurmountable logical flaws in the design.
To analyze the networking dynamics and resource consumption, we studied our implementations through a set of experiments described in the next section.

\subsection{Methodology}
\label{subsec:methodology}

Experiments were conducted using the Tor shadow simulator tool~\cite{jansen2011shadow, tracey2018high}.
We ran two sets of experiments at different scales from a consensus document published in early February 2018.
The first set featured 100 relays, 1000 clients, 10 intermediaries, and ran for a total of 90 minutes.
These experiments were used to gather information concerning the system overhead and protocol execution times.
The second set featured 250 relays, 2500 clients, 25 intermediaries, 80 minutes of total run time, and was used to measure the performance benefits conferred to premium clients.
In both cases, simulated traffic consists of 8\% \emph{bulk} clients who continuously download 5 MiB files and 92\% \emph{web} clients who periodically download 2 MiB files.\footnote{While 5 MiB bulk files are a common standard in Tor benchmarking~\cite{portal2018tormetrics}, 2 MiB web files reflect the approximate size of modern web pages~\cite{team2018httparchive}.}
The number and behavior of clients were chosen to satisfy (A) realistic congestion rates measured by a transfer timeout percentage of approximately 4\%~\cite{portal2018tormetrics} and a historical bulk/web global traffic ratio of about 1:3~\cite{privcount-ccs2016, learning-ccs2018}.
Neither the scale of our experiments nor the precise configuration of client nodes are intended to be precise replicas of real-world conditions.
Tor networking is itself a complex area of research and, for our purposes, we are content to adopt the simplest model that will highlight the relatively crude networking needs of our incentivization scheme.

\subsection{Experiments}

\label{subsec:experiments} Our experiments are separated into three groups: global overhead, payment latency, and network priority.
Each captures a separate characteristic of the scheme.

\medskip \noindent \textbf{Global Overhead.}
First, we attempt to show the total cost of the moneTor scheme in terms of total network throughput.
To study worst-case performance, we configured a medium scale experiment consisting of 100\% premium clients which we compared to a baseline trial with 0\% premium clients.
The purpose of this experiment was to measure the worst-case overhead imposed by the payment scheme \emph{without} applying any network prioritization in either control-flow or EWMA.
Since our protocol can benefit from concurrently executed cryptographic operations, a key parameter to the simulation is the number of CPU cores available on each relay.
Unfortunately, this information is not publicly available.
As a result, we conducted two trials: one in which all nodes are running on multi-core hardware and one in which all nodes are running on single-core hardware.
Figure~\ref{fig:overhead} summarizes the results.

\begin{figure*}[t] \centering
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[trim={0 0cm 0 0cm}, clip, width=1.0\textwidth]{images/payment_establish.pdf}
    \caption{Nano-Establish - Built after the circuit construction, but before the circuit usage!}
    \label{fig:payments_establish}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[trim={0 0cm 0 0cm}, clip, width=1.0\textwidth]{images/payment_pay.pdf}
    \caption{First Payment - Should match 1 half RTT if the preemptive Nano-Establish is perfect}
    \label{fig:ttfp}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[trim={0 0cm 0 0cm}, clip, width=1.0\textwidth]{images/payment_close.pdf}
    \caption{Nano-Close - Happens just before the circuit is destroyed}
    \label{fig:payments_close}
  \end{subfigure}
  \caption{Protocol Execution Time --- Time to finish each protocol step split across interactions with each of the three relays.
    The simulation includes 100 relays, 2 authorities, 1 ledger authority, 10 intermediaries and 1000 Tor clients scaled down from the public consensus file `2018-02-03-00-00-00-consensus'.}
  \label{fig:latencymeasurements}
\end{figure*}

Our findings indicate that even in the worst case scenario, our system incurs statistically negligible overhead at these scales across the measures of download time (e.g., less than 2\% increase on the mean web download for the singlecore experiment), throughput, and memory usage.
When examining the raw network messages, we found corroborating evidence that moneTor contributes to only a small fraction, less than $1\%$, of the total network traffic in our experiment, a result which holds true across all of our trials.
By default, we configured a payment rate of one payment cell for every 1000 data cells exchanged in either direction.
If the network requires more fairness, it is also possible to increase the payment rate with negligible CPU cost as long as the network overhead introduced by the control cells remains under an acceptable fraction of the overall bandwidth.

\medskip \noindent \textbf{Payment Latency.}
Given the results from our experiments, we surmise that payment latency is a crucial factor in servicing front-loaded clients.
To this end, we measure the distribution of completion times for various steps in the protocol.
To highlight the effects of native latency in the Tor network, we show payments split across each relay role of guard, middle, and exit.
Recall that moneTor makes use of high-overhead, low-marginal cost payment channels (i.e., the channels take time to build but the client needs them long after they are built).
In other words, the bulk of the cost in our scheme lies in the execution of \textbf{Nano-Establish} and \textbf{Nano-Close} protocols as shown in Figure~\ref{fig:payments_establish} and Figure~\ref{fig:payments_close}.

Notice that nano-close operations take roughly twice as long to complete as the nano-establish operations due to the need for the relay to close his half of the nanopayment channel before the client can complete hers.
Figure~\ref{fig:ttfp} illustrates the time to first payment, our most revealing latency metric.
This measure includes the overhead in channel establishment when we do not have available preemptive channels.
In the best case scenario, when all three payment channels have been correctly pre-built for the circuit, this measure is equivalent to a single trip toward each relay.
Comparing this Figure~\ref{fig:ttfp} to Figure~\ref{fig:payments_establish}, we observe the effectiveness of preemptive channel building.
The other observation supporting the effectiveness of the pre-built strategy is the recorded time for the ``call'' versus ``send'' lines; if no discrepancy is observed between them, it means that the pre-build successfully led to fully established channels.

In all protocol phases, we observe that latencies for guard relays are negligible in comparison to the middle and exit relays, which is a result of our design decision to implement directly-paid guard channels.
Again, this is a Tor-specific optimization made possible by the fact that guards maintain a semi-persistent, transparent relationship with only a small subset of clients.

\medskip \noindent \textbf{Network Priority.}
\label{sec:priority_exp} Our final set of experiments studies the success of our scheme in delivering prioritized traffic for premium users.
To perform this analysis, we prepared sets of three small experiments with varying modifier priorities: $\alpha \in \{0, 0.25, 0.5\}$ and $\beta \in \{1, 5, 10\}$.
From a networking perspective, $\alpha = 0, \beta = 1$ is equivalent to unmodified Tor.
We set the fraction premium users to be 25\%.
From Figures \ref{fig:modifier_pr25_web}, \ref{fig:modifier_pr25_bulk}, and \ref{fig:modifier_pr25_all}, we observe that, first, variations in our network-wide tunable parameters do offer differentiation in download speed.
Yet, as we detail in Appendix~\ref{sec:scheduling}, offering bandwidth differentiation for the Tor network is more complex than previously assumed.
Indeed, local scheduling priority, which was historically effective for past Tor topologies, appears to be ineffective under current conditions where congestion is concentrated at the exit interface.
Second, the differentiation in bandwidth for $\alpha = .25, \beta=5$ ``averages out'' to approximately mirror the baseline experiment, indicating little loss in overall network performance, and confirming our overhead experiment (recall 25\% of premium users).
Nevertheless, our result for $\alpha = 0.5, \beta=10$ indicates that the performance degrades faster for nonpremium users when we become too aggressive in procuring gains for premium users.
The data also highlights the complexity in selecting a set of parameters and techniques to offer efficient prioritization without degrading the overall throughput of the network.
The overarching takeaway is that the network prioritization mechanism appears to be an even more complex challenge than the design of the anonymous payment layer itself.

Note that our analysis of the scheme holds the total network capacity static.
However, the motivation for any Tor incentivization scheme is to attract new relays to grow the network which would, in principle, improve anonymity and censorship resistance for all users.
The effect on performance is less clear.
Although adding new relays will increase the throughput, a faster Tor would likely attract more users as well.
In the absence of a reliable economic model, it is unclear how incentives would affect the experience of the average user, and so we opted to forgo modeling the added capacity.

\begin{figure*} \centering
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[trim={0 0cm 0 0cm},
      clip,width=1.0\textwidth]{images/modifier_pr25_web_lowloss.pdf}
    \caption{Web Download Time}
    \label{fig:modifier_pr25_web}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[trim={0 0cm 0 0cm}, clip,
      width=1.0\textwidth]{images/modifier_pr25_bulk_lowloss.pdf}
    \caption{Bulk Download Time}
    \label{fig:modifier_pr25_bulk}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth} \centering
    \includegraphics[trim={0 0cm 0 0cm}, clip,
      width=1.0\textwidth]{images/modifier_pr25_all_lowloss.pdf}
    \caption{Total throughput, all clients}
    \label{fig:modifier_pr25_all}
  \end{subfigure}
  \caption{Prioritization Benefit --- Performance differentiation between paid and unpaid users.
    We display results for 25\% premium users.
    Simulations feature 250 relays, 2 authorities, 1 ledger authority, 25 intermediaries and 2500 Tor clients scaled down from the public consensus file `2018-02-03-00-00-00-consensus'.}
  \label{fig:modifier}
\end{figure*}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../popets_monetor"
%%% End:
