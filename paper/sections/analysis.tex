To validate our design decisions, began with analysis of data collected from
real-world Tor users. \op{Is it only validation? It seems that some of the paramters are also chosen based on these simulations. Also, the discussion seems to be using some parameters which are only made explicit later. For instance, in 7.1.2, the circuits with less than 1000 cells are described as useless for premium. The reson for that seems to only become clear when reading 8.3.1, which indicates that payments are made after 1000 cells. I wonder if, at the end of this section, and before the experimental analysis, it would make sense to add a frame that sums-up what protocol will be analyzed, including the parameters: what channels and circuits are created when, when is payment happening, when are channels closed, \dots}

\subsection{Data Collection}
\label{subsec:datacollection}

We deployed during two weeks a data collection system to look for empirical temporal information
about lifetime and bandwidth consumption in Tor circuits. Our objective was to
have a deeper understanding of typical Tor usage and whether such usage can
benefit from our channel-based payment system. For example, these measurements
might capture some notion about the type and magnitude of potential premium
traffic. We classify the traffic type based on the service connection
port. Besides the classical ports 80 and 443 used for web traffic, we aggregate
data from some other families including the WHOIS
protocol~\cite{daigle2004whois} and RWHOIS~\cite{williamson1994referral} from
ports 43 and 4321, respectively. The complete list of families is constructed
from the reduced exit policies which we run on our relays. This measurement
methodology allows us to reason based on application specific traffic.

% We interested to know about the distribution lifetime of Tor circuits for each
% port we allow. We are also interested to picture how many cells those circuits
% handled through their lifetime with some level of granularity.

\subsubsection{Efforts to preserve users privacy}

To ensure ethical experimentation, we first contacted the Tor research safety
board~\cite{torsafety}. The feedback we received was subsequently used to refactor our data
collection process.

Data from multiple old and very stable exit relays was collected, stripped of
origin metadata, and aggregate on a central server. The data collected from each
relay is itself an aggregation which we perform inside the relay's memory. The
data collection is probabilistic; only 30\% of the circuits processed by our
relays were considered in order to maintain plausible deniability on the
clients' behalf. The aggregation is done inside bins of configurable size for
every different traffic family we consider. Once we collected enough data from a
single family, we dump the information on the disk, clear the data, and resume a
new session. The final information obtained contains an aggregation of 1600
circuits over an unspecified time frame that is implicitly determined by the
rate of user activity. The following data was considered:

\begin{itemize}
\item \textbf{Time Profile}: The number of cells in each time interval
  (configured to be 5 seconds) since the success of the DNS request. This
  information sums inbound and outbound cells, and is aggregated over circuits
  by addition.
\item \textbf{Total Counts}: The total amount of cells processed by a
  circuit. This information is aggregated by taking the mean of fixed-size
  nearest neighbor bins.
\end{itemize}

Crucially, we do not record information linked to any single particular user
flow on disk.
%The code used for the data collection will be made available for
%audit online.

\subsubsection{Observations}

\begin{figure} \centering
  \includegraphics[width=0.4\textwidth]{images/exitmeasurement.png}
  \caption{Time Profile --- Average distribution of traffic across a circuits
    lifetime beginning with the first successful DNS request.}
  \label{fig:statsa}
\end{figure}
\begin{figure} \centering
  \includegraphics[width=0.4\textwidth]{images/totcellcountscdf.png}
  \caption{Total Counts --- Distribution of circuit size with respect to the
    number of cells processed across the circuit lifetime}
\label{fig:statsb}
\end{figure}


Our measurements successfully captured several important pieces of information
for the design and justification of moneTor. For example, one important task is
to determine the number of potential users that could benefit from paid
traffic. From Figure~\ref{fig:statsb}, we observe that $\approx 82\%$ of
circuits carrying only web traffic exchanged less than 1000 cells. While we
cannot deduce any statements about users, we can speak to the fraction of
circuits that may benefit from a payment channel in the Tor network, since
around $50\%$ of them do not carry data and less than $17\%$ of them carry at
least one web page. The remaining $18\%$ would appear to be better candidates
for moneTor.

It is also evident from Figure~\ref{fig:statsa} that most of the traffic usually
happens within the first few tens of seconds, and that all types of traffic we
collected seems to follow the same rule. From that result, we believe that the
reliability of payment is critical within the first few seconds, especially from
a relay viewpoint. Our payment channels should ideally be established and ready
before the user begins to use the circuit. While this result cannot be
guaranteed for an unbounded number of circuits, a well-designed preemptive
circuit build strategy should do a sufficient job of eliminating channel
setup/establish latency in the average case.

%% In another area of research, it may be interesting to point out that since %
%$\approx 50\%$ of users do not carry data after their DNS request, some %
%adversary doing end-to-end correlation may prefer to use active attacks over %
%passive correlation to capture more identities.

\section{Simulated Validation}
\label{sec:experimentations}

Having established the empirical context for a channel payment scheme, we
validated our technical design via experiments performed on a proof-of-concept
software implementation within the native Tor codebase.

\subsection{Prototype}

A substantial contribution of the research is embedded within our implementation
of the moneTor framework. The modifications, applied to Tor release version
0.3.2.10, cover approximately fifteen thousand added lines of code across Tor's
core C software. We emphasize that the implementation is not a fully functional
prototype and is optimized solely for our experiments. Most notably, we
simulated crytographic operations in the nanopayment creation and close
procedures using cpu delays. These delays were tuned to conservatively reflect
real measurements in background work~\cite{green2017bolt}.\footnote{Extracted
  values are conservative in the sense that our zero-knowledge proofs require
  proving only a subset of the statements required in each corresponding Bolt
  zero-knowledge proof.}  In spirit, the partial prototype serves the following
purposes in our study.

\begin{enumerate}
\item A proof-of-concept implementation covering nuances not explicitly
  covered in the protocol designs. In effect, we would like to show that there
  are no unexpected and prohibitive practical conflicts with the existing Tor
  design.
\item A platform to study the feasibility of premium circuit
  prioritization from a networking perspective.
\item A platform to obtain a rough factor-of-two approximation for all
  bandwidth, computation, and memory requirements of the system, both globally
  and at individual nodes.
\end{enumerate}

The first design purpose is clearly qualitative and we briefly note that we did
not discover any insurmountable logical flaws in the design. To analyze the
networking dynamics and resource consumption, we studied our implementations
through the following proceeding experiments.

\subsection{Methodology}
\label{subsec:methodology}

Experiments were conducted using the Tor shadow simulator
tool~\cite{jansen2011shadow}. We ran two sets of experiments at different
scales from a consensus document published in early February 2018. The first set featured 100 relays, 1000 clients, 10 intermediaries, and
ran for a total of 90 minutes. These experiments were used to gather information
concerning the system overhead and protocol execution times. The second set
featured 250 relays, 2500 clients, 25 intermediaries, 80 minutes of total run
time, and was used to measure the performance benefits conferred to premium
clients. In both cases, simulated traffic features 16\% \emph{bulk} clients who
continuously download 5 MiB files and 84\% \emph{web} clients who periodically
download 2 MiB files,\footnote{While 5 MiB bulk files are a common standard in
  Tor benchmarking~\cite{portal2018tormetrics}, 2 MiB web files reflect the
  approximate size of modern web pages~\cite{team2018httparchive}}. The number
and behavior of clients were chosen to satisfy (A) realistic congestion rates
measured by a transfer timeout percentage around 4\%~\cite{portal2018tormetrics}
and a historical bulk/web global traffic ratio of about
50\%~\cite{chaabane2010digging, mccoy2008shining}. Importantly, it should be
noted that neither the scale of our experiments nor the precise configuration of
client nodes are intended to be precise replicas of real-world conditions. Tor
networking is itself a complex area of research and we are content to adopt the
simplest model that will highlight the relatively crude networking needs of our
incentivization scheme.

\subsection{Experiments}
\label{subsec:experiments}
Our experiments are separated into three groups each capturing a separate
characteristic of the scheme.

\subsubsection{Global Overhead}

First, we attempt to show the total cost the moneTor scheme in terms of total
network throughput. To highlight worst-case performance, we configured a medium
scale experiment consisting of 100\% of premium clients and compare to a
baseline trial with 0\% premium clients. No actual traffic priority is
conferred. Since our algorithms makes use of some concurrent crytpographic
operations, we are concerned with the number of CPU cores available to most
relays. This information is not publicly available. As a result, we performed
provide two trials with moneTor: one where we assume all nodes are running on
multi-core hardware and one in which all nodes are running on single-core
hardware. The results are summarized in Figure~\ref{fig:overhead}.

\begin{figure*} \centering
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/overhead_downloadtime.pdf}
		\caption{Download Time Overhead}
		\label{fig:overhead_ttlastbyte}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/overhead_throughput.pdf}
		\caption{Throughput Overhead}
		\label{fig:overhead_throughput}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/overhead_memory.pdf}
		\caption{Simulation Memory}
		\label{fig:overhead_shadow}
	\end{subfigure}
	\caption{Global Overhead --- Comparison of results run across a
          baseline, multicore moneTor, and singlecore moneTor
          trials. Figure~\ref{fig:overhead_ttlastbyte} shows two sets of
          download time CDF curves for each file size (2 MiB and 5 MiB),
          Figure~\ref{fig:overhead_throughput} shoes the 5 minute moving average
          throughput over time, and Figure~\ref{fig:overhead_shadow} shows the
          memory consumption across the experiment lifetime. The simulation
          includes 100 relays, 2 authorities, 1 ledger authority, 10
          intermediaries and 1000 Tor clients scaled down from the public
          consensus file `2018-02-03-00-00-00-consensus'.}
	\label{fig:overhead}
\end{figure*}

Our findings indicate that even in the worst case scenario, our system incurs
statistically negligible overhead at these scales across all three measures of
download time, throughput, and memory usage. This in line with the raw bandwidth
overhead observations in which a small fraction ($< 1\%$) of the network traffic
is attributable to moneTor payment messages. This is true for all of our trials
which feature a payment rate of 1 payment (1 cell) every 1000 data cells
exchanged in either direction. It is also possible to increase the payment rate
for more fairness if needed, as long as the total overhead induced from control
cells is kept under an acceptable fraction of the overall data bandwidth. This
low overhead cost is separate from adverse networking effects of prioritization,
which has the potential to more drastically affect performance.

%\begin{table}
%  \caption[Overhead Throughput Total]{\textbf{Overhead Throughput Total} Total
%    transferred application traffic over the duration of the over head trials
%    compared to total transferred payment traffic.}
%  \begin{center}
%    \begin{tabular}{ c c c }
%      & Throughput (GiB) & Payment Traffic (GiB) \\ \hline
%      Baseline & 128.4 & 0.000 \\
%      Multi-Core & 129.1 & 0.449 \\
%      Single-Core & 127.6 & 0.396
%    \end{tabular}
%  \end{center}
%  \label{tab:overhead}
%\end{table}

\subsubsection{Payment Latency}

Given results from our data collection, we surmise that payment latency is a
crucial factor in servicing our front-loaded clients. To this end, we measure
the distribution of completion times for various steps in the protocol. The
results shown here are collected from a worst case multicore experiment
featuring 100 relays and 1000 clients all of whom run premium circuits. To
highlight the effects of native latency in the Tor network, we show payments
split across each relay role of guard, middle, and exit. Recall that moneTor
makes use of high-overhead, low-marginal cost payment channels. The bulk of the
cost in our scheme lies in the conduction of the nanopayment channel
\emph{establish} and \emph{close} protocols as shown in
Figure~\ref{fig:payments_establish} and Figure~\ref{fig:payments_close}. Notice
that close operations are about twice as time consuming as establish operations
reflecting the need for the relay to close his half of the nanopayment channel
before the client can complete hers. Figure~\ref{fig:ttfp} illustrates time to
first payment, our most revealing latency metric. This measure includes the
overhead in channel establishment when we do not have available preemptive
channels. In the best case scenario, when all three payment channels have been
correctly pre-built for the circuit, this measure is equivalent to a single trip
toward each relay. Comparing this Figure~\ref{fig:ttfp} to
Figure~\ref{fig:payments_establish}, we observe the effectiveness of preemptive
channel building.

\begin{figure*} \centering
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/payment_establish.pdf}
		\caption{Nano-Establish}
\label{fig:payments_establish}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/payment_pay.pdf}
		\caption{First Payment}
\label{fig:ttfp}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/payment_close.pdf}
		\caption{Nano-Close}
\label{fig:payments_close}
	\end{subfigure}
	\caption{Protocol Execution Time --- Time to finish each protocol step
          split across interactions with each of the three relay. The simulation
          includes 100 relays, 2 authorities, 1 ledger authority, 10
          intermediaries and 1000 Tor clients scaled down from the public
          consensus file `2018-02-03-00-00-00-consensus'.}
\label{fig:latencymeasurements}
\end{figure*}

In all protocol phases, we observe that latency for guard relays are negligible
in comparison to the middle and exit relays, further validating our design
decision to implement special directly paid guard channels.

\subsubsection{Network Priority}
\label{sec:priority_exp}

Our final set of experiments studies the success of our scheme in delivering
prioritized traffic for premium users. To perform this analysis, we prepared
sets of three small experiments with varying modifier priorities
$\alpha \in \{0, 0.25, 0.5\}$, where $\alpha = 0$ represents the baseline
control. Our first set of experiments assuming 50\% of premium users is shown in
Figures \ref{fig:modifier_pr50_web}, \ref{fig:modifier_pr50_bulk}, and
\ref{fig:modifier_pr50_all}. From this illustration, is it evident that premium
users enjoy an advantage in internet speed compared to nonpremium users and that
this advantage is more pronounced with the higher priority modifier. Moreover,
the evenly split premium and nonpremium performance ``averages out'' to
approximately mirror the baseline experiment, indicating little loss in overall
network performance, and confirming our overhead experiment.

A more realistic model should most likely assume a smaller minority of premium
users. We therefore conducted a separate set of experiments to study an
environment comprised of only 25\% premium users who, according to
Equation~\ref{eq:flow}, should expect an even greater benefit on a per-user
basis. Figures \ref{fig:modifier_pr25_web}, \ref{fig:modifier_pr25_bulk}, and
\ref{fig:modifier_pr25_all} shows that this is indeed the result. Users across
the spectrum of web speed enjoyed $\approx 100\%$ improvements in download
speeds relative to nonpremium users --- likely enough to induce some amount of
monetary exchange.

\begin{figure*} \centering
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/modifier_pr50_web.pdf}
		\caption{Web Download Time}
\label{fig:modifier_pr50_web}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/modifier_pr50_bulk.pdf}
		\caption{Bulk Download Time}
\label{fig:modifier_pr50_bulk}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/modifier_pr50_all.pdf}
		\caption{Throughput}
\label{fig:modifier_pr50_all}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/modifier_pr25_web.pdf}
		\caption{Web Download Time}
\label{fig:modifier_pr25_web}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/modifier_pr25_bulk.pdf}
		\caption{Bulk Download Time}
\label{fig:modifier_pr25_bulk}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[trim={0 3cm 0 3cm}, clip, width=1.0\textwidth]{images/modifier_pr25_all.pdf}
		\caption{Throughput}
\label{fig:modifier_pr25_all}
	\end{subfigure}
	\caption{Prioritization Benefit --- Performance differentiation between
          paid and unpaid users. The first row displays results with 50\%
          premium users while the second row displays results for 25\% premium
          users. Simulations feature 250 relays, 2 authorities, 1 ledger
          authority, 25 intermediaries and 2500 Tor clients scaled down from the
          public consensus file `2018-02-03-00-00-00-consensus'.}
\label{fig:modifier}
\end{figure*}

%\begin{table}
%  \caption[Prioritized Throughput]{\textbf{Prioritized Throughput} Tabulate total
%    number of successful file transfers and timeout errors across varying levels
%    of prioritization}
%  \begin{center}
%    \begin{tabular}{ c c c c}
%      & Throughput & Timeouts & Timeout \\
%      & (GiB) & (Web) & (Bulk) \\ \hline
%      $\alpha = 0.00$ & 427 & 115 & 354 \\ \hline
%      $\alpha = 0.25,\ pr\% = 50$ & 429 & 206 & 404 \\
%      $\alpha = 0.25,\ pr\% = 25$ & 433 & 231 & 459 \\ \hline
%      $\alpha = 0.50,\ pr\% = 50$ & 420 & 233 & 356 \\
%      $\alpha = 0.50,\ pr\% = 25$ & 414 & 381 & 513 \\
%    \end{tabular}
%  \end{center}
%  \label{tab:modifier}
%\end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
