To validate our design descisions, we used a two-pronged approach that consists
of both real-world data collection from live Tor users along with simulations on
our software implementation.

\subsection{Data Collection}
\label{subsec:datacollection}

We deployed a data collection system to look for empirical temporal information
about lifetime and bandwidth consumption Tor circuits. Our objective is to have
a deeper understanding of typical Tor usage and whether such usage can benefit
from our channel-based payment system. For example, these measurements might
capture some notion about the type and magnitude of potential premium
traffic. We classify the traffic type based on the service connection
port. Besides the classical ports 80 and 443 used for web traffic, we aggregate
data from some other families including the WHOIS protocol~\cite{rfc3912} and
RWHOIS~\cite{rfc2167} from ports 43 and 4321, respectively. The complete list of
families is constructed from the reduced exit
policies~\cite{reducedexitpolicies} which we run on our relays. This measurement
methodology allows us to reason based on application specific traffic.

% We interested to know about the distribution lifetime of Tor circuits for each
% port we allow. We are also interested to picture how many cells those circuits
% handled through their lifetime with some level of granularity.

\subsubsection{Efforts to preserve users privacy}

To ensure ethical experimentation, we first contacted the Tor research safety
board, a group of Tor researchers who aim minimize privacy risks while fostering
a better understanding of the Tor network and its users~\cite{torsafety}. The
feedback we received was subsequently used to refactor our data collection
process.

Data from multiple exit relays was collected, stripped of origin metadata, and
aggregate on a central server. The data collected from each relay is itself an
aggregation which we perform inside the relay's memory. The data collection is
probabilistic, only 30\% of the circuits processed by our relays were considered
to maintain plausible deniablity on the client's behalf. The aggregation is done
inside bins of configurable size for every different traffic family we
consider. Once we collected enough data from a single family, we dump the
information on the disk, clear the data, and resume a new session. The final
information dumped contains an aggregation of 1600 circuits over an unspecified
time frame that is implicitly determined by the rate of user activity. The
following data was considered:

\begin{itemize}
\item \textbf{Time Profile}: The number of cells in each time interval
  (configured to be 5 seconds) since the success of the DNS request. This
  information sums inbound and outbound cells, and is aggregated over circuits
  by addition.
\item \textbf{Total Counts}: The total amount of cells processed by a
  circuit. This information is aggregated by taking the mean of fixed-size
  nearest neighbor bins, such that the precision of the recorded value depends
  on the number of bins.
\item \textbf{Time Stdevs}: The standard deviation accross the time profiles of
  individual circuits. This information is aggregated in a similar method as the
  Total Counts. This statistic shed some information on the temporal
  deviation of user activity, which can loosely be used to infer interactive vs
  noninteractive traffic.
\end{itemize} Crucially, we do not record information linked to any single
particular user flow on disk. The code used for the data collection is available
online, and can be audited~\cite{code-mt-stats}.

\subsubsection{Observations}

Our measurements successfully captured several important informations needed to
design moneTor and our experimentation phase. For example, one important question would be:
how should we choose the fraction of premium users in our experimentations? From
Figure~\ref{fig:stats_b}, we observe that $\approx 82\%$ of circuits carrying
only web traffic exchanged less than 1000 cells for both directions (i.e.,
toward the web server and toward the user). This metric does not say that $82\%$
of Tor webusers use less than 1000 cells for a web connection, since it is
theoretically possible that only one user is responsible for consuming less than
1000 cells across our measurements. However, it gives a good idea of the
fraction of circuits that may benefit from a payment channel in the Tor network,
since around $50\%$ of them do not carry data and less than $17\%$ of them carry
at least one web page. These numbers do not mean that $\approx 82\%$ of circuits
carry unethical network usage.

It also appears from Figure~\ref{fig:stats_a} that most of the traffic usually
happens within the first few tens of seconds, and that all type of traffic we
collected seems to follow the same rule. From that result, we believe that the
reliability of payment is critical within the first few seconds, especially from
a relay side viewpoint. This result means that our payment channels must be
established and ready before the user starts to use his circuit. Tor already
follows the same strategy with streams: a circuit must be ready before a stream
is created, such that it can be attached to a ready circuit and sends its data
directly through the anonymous tunnel. Similarly, moneTor allows secure and
anonymous payment channels to be pre-built and attached to a particular
circuit. However, Tor would need some clever strategy not to exhaust the
network's resource by pre-building unnecessary nanopayment channels.

%% In another area of research, it may be interesting to point out that since %
%$\approx 50\%$ of users do not carry data after their DNS request, some %
%adversary doing end-to-end correlation may prefer to use active attacks over %
%passive correlation to capture more identities.

\begin{figure*} \centering
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.3]{images/exitmeasurement.png}
		\label{fig:stats_a}
		\caption{Time Profile}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.3]{images/totcellcountscdf.png}
		\label{fig:stats_b}
		\caption{Total Counts}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.3]{images/stddevs.png}
		\label{fig:stats_c}
		\caption{Time Stdevs}
	\end{subfigure}
	\label{fig:measurements}
	\caption{Tor measurements}
\end{figure*}

\subsection{Simulation Methodology}

A substantial contribution of the research is embedded within our native
implementation of the moneTor framework. The modifications, applied to Tor
version 0.3.2-alpha, cover approximately fifteen thousand added lines of code
across Tor's core C codebase. We emphasize that the implementation is not a
fully functional prototype and is optimized solely for our experiments. Indeed,
the implementation is designed exclusively to serve the following purposes:

\begin{enumerate}
\item A proof-of-concept implementation covering nuances not explicitly
  covered in the protocol designs. In effect, we would like to show that there
  are no unexpected and prohibitive practical conflicts with the existing Tor
  design.
\item A platform to study the feasibility of premium circuit
  priorization from a networking perspective.
\item A platform to obtain a rough factor-of-two approximation for all
  bandwidth, computation, and memory requirements of the system, both globally
  and at individual nodes.
\end{enumerate}

The first design purpose is clearly qualitative. Although we will discuss
lessons learned in~\ref{discussion?}, we briefly note that we did not discover
any unsurmountable logical flaws in the design. Meanwhile, in order to analyze
the networking dynamics and resource consumption, we studied our implementations
through a series of experiments.

\subsection{Experiments}

The experiments were conducted using the Tor shadow simulator
tool~\cite{jansen2011shadow} described in Section~\ref{sec:background}. We
approach the problem at multiple scales. Given the deterministic and virtual
nature of the simulator, the choice of hardware does not affect our results.

\subsubsection{Medium-scale Parameter Tuning}

The moneTor scheme is parametized by several core variables that strongly affect
network performance. In particular, we are interested in how the following
variables can affect the benefit of priority bandwidth clients.

\begin{itemize}
\item \emph{Priority Modifier}: priority benefit can be tuned with a configurable
  program option.
\item \emph{Congestion}: priority benefit is likely more prominent when there is
  competition for limited bandwidth.
\item \emph{Percentage Premium Users}: priority benefit may be less prominent
  when there are more prioritized users.
\end{itemize}

To measure the effect of these independent variables, we run a large number of
medium-scale simulations. Each parameter is varied individually while keep the
other two constant at an arbitrary acceptable value. For each simulation, we
take as the dependent variable the global median download time for all premium
clients and all non-premium clients, respectively~\ref{figures}.

\td{Note: we haven't actually done this yet. Figures are forthcoming'}

The sum outcome of these experiments allow us comment on the sensitivity of our
scheme to each parameter and to recommend optimal configurations for
larger-scale analysis and deployment.

\subsubsection{Large-Scale Performance Analysis}

Given the preceeding results, we choose a single \td{or several?} large-scale
experimental configuration with which to analyze the performance in more
detail. Our large-scale experiment features the following setup \td{describe
  setup}

\td{Experimental results forthcoming. We expect to have graphs that are simular
  to the very small scale placeholders shown in Figure 3 and Figure 4. Figure 3
  shows payment statistics where we basically observe latencies for various
  payment steps split between guard, middle, and exit relays. Figure 4 shows
  network effects where we look at throughput (to infer overhead differences
  between moneTor and vanilla), time to first byte for
  premium/nonpremium/vanilla clients, and time to finish download for
  premium/nonpremium/vanilla clients in order to get a more detailed look into
  the benefits of premium traffic}

\begin{figure*} \centering
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.2]{images/numpayments_temp.png}
		\label{fig:payments_a}
		\caption{Number of Payments}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.2]{images/ttestablish_temp.png}
		\label{fig:payments_b}
		\caption{Time to Establish}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.2]{images/ttclose_temp.png}
		\label{fig:payments_c}
		\caption{Time to Close}
	\end{subfigure}
	\label{fig:payments}
	\caption{Payment Simulations (Temporary Placeholder)}
\end{figure*}


\begin{figure*} \centering
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.2]{images/throughput_temp.png}
		\label{fig:payments_a}
		\caption{Network Throughput}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.2]{images/ttfirstbyte_temp.png}
		\label{fig:payments_b}
		\caption{Time to First Byte}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth} \centering
\includegraphics[scale=0.2]{images/ttdownload_temp.png}
		\label{fig:payments_c}
		\caption{Time to Complete Download}
	\end{subfigure}
	\label{fig:payments}
	\caption{Network Simulations (Temporary Placeholder)}
\end{figure*}
